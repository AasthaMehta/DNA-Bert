# -*- coding: utf-8 -*-
"""Copy of BERTM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14JmuaCH3fBn9j5E_8-kCKhOCS3dYLZLS
"""



!pip install transformers
!pip install scikit-learn

from google.colab import drive
drive.mount('/content/drive')

import torch
print(torch.__version__)

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split

# Load training data from CSV file (replace 'chipseq.csv' with your file location)
train_data = pd.read_csv('/content/text1.csv')

train_data

# Split the data into sequences (X) and expression levels (y)
sequences = train_data['DNA'].values
labels = train_data['Target'].values

# Split data into training and validation sets
train_sequences, val_sequences, train_labels, val_labels = train_test_split(
    sequences, labels, test_size=0.1, random_state=42
)

sequences[:10]

labels

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence  # Add this line for pad_sequence
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error



# Split data into training and validation sets
train_sequences, val_sequences, train_labels, val_labels = train_test_split(
    sequences, labels, test_size=0.1, random_state=42
)


# Modify the GeneExpressionDataset class for regression to handle data loading and preparation for both training and testing
class GeneExpressionRegressionDataset(Dataset):
    def __init__(self, sequences, labels, tokenizer, max_length=None):
        self.sequences = sequences
        self.labels = labels.astype(np.float32)  # Ensure labels are in float format
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        sequence = str(self.sequences[idx])
        label = self.labels[idx]

        # Tokenize input sequences
        encoding = self.tokenizer(sequence, truncation=True, padding=True, max_length=self.max_length, return_tensors='pt')

        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'label': torch.tensor(label)  # Return a single numerical label for regression
        }

# Initialize BERT tokenizer and model for regression
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)  # Regression with a single output

# Use torch.optim.AdamW instead of transformers.AdamW
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)  # Adjust learning rate

# Define loss function for regression (e.g., Mean Squared Error)
loss_fn = torch.nn.MSELoss()  # Use MSELoss for regression

# Define a collate function to handle dynamic padding
def collate_fn_regression(batch, tokenizer):
    input_ids = [item['input_ids'] for item in batch]
    attention_masks = [item['attention_mask'] for item in batch]
    labels = [item['label'] for item in batch]

    # Pad sequences to the length of the longest sequence in the batch
    max_len = max(len(seq) for seq in input_ids)
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)  # Assuming attention mask is 0 for padded tokens

    return {
        'input_ids': input_ids,
        'attention_mask': attention_masks,
        'labels': torch.stack(labels)
    }

# Create training dataset and data loader with collate_fn_regression
train_dataset = GeneExpressionRegressionDataset(train_sequences, train_labels, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=lambda batch: collate_fn_regression(batch, tokenizer))

# Create validation dataset and data loader with collate_fn_regression
val_dataset = GeneExpressionRegressionDataset(val_sequences, val_labels, tokenizer)
val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=lambda batch: collate_fn_regression(batch, tokenizer))

# Training loop for regression task
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for batch in train_loader:
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs.logits.squeeze(1), labels)  # Calculate loss for regression task
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            outputs = model(input_ids, attention_mask=attention_mask)
            loss = loss_fn(outputs.logits.squeeze(1), labels)  # Calculate loss for regression task

            val_loss += loss.item()

    # Calculate average training and validation loss for the epoch
    avg_train_loss = train_loss / len(train_loader)
    avg_val_loss = val_loss / len(val_loader)

    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')

# At this point, you can save the model or use it for inference on new data


# Training loop
best_val_loss = float('inf')
best_epoch = 0
patience = 5  # Set your desired patience value

# Save the model checkpoint if validation loss improves
checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/model_checkpoint.pth'

# Then use it in your code
if avg_val_loss < best_val_loss:
    best_val_loss = avg_val_loss
    best_epoch = epoch
    torch.save(
        {
            "epoch": epoch + 1,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "train_loss": avg_train_loss,
            "val_loss": avg_val_loss,
        },
        checkpoint_path,
    )
else:


  if epoch - best_epoch > patience:
    print(f"Early stopping at epoch {epoch + 1}")
     # Ensure the break statement is within the else block



# Load the best model checkpoint
checkpoint = torch.load(checkpoint_path)
model.load_state_dict(checkpoint['model_state_dict'])

# Save the best model checkpoint to Google Drive
google_drive_checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/fine'
torch.save({
    'epoch': best_epoch + 1,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'train_loss': best_val_loss,  # Save the validation loss as the best loss
    'val_loss': best_val_loss
}, google_drive_checkpoint_path)

print(f'Model saved to {google_drive_checkpoint_path}')

# Define a collate function for regression
def collate_fn_regression(batch, tokenizer):
    input_ids = [item['input_ids'] for item in batch]
    attention_masks = [item['attention_mask'] for item in batch]
    labels = [item['label'] for item in batch]

    max_len = max(len(seq) for seq in input_ids)
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)

    labels = torch.stack(labels)  # Convert labels to a tensor

    return {
        'input_ids': input_ids,
        'attention_mask': attention_masks,
        'labels': labels
    }

import ast
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

# Load test data from CSV file
test_data = pd.read_csv('/content/text1.csv')

test_sequences = test_data['DNA'].values
test_labels = test_data['Target'].values  # Assuming 'labels' column contains single numbers
max_length=102
# Create test dataset and data loader
test_dataset = GeneExpressionRegressionDataset(test_sequences, test_labels, tokenizer, max_length=max_length)
test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=lambda batch: collate_fn_regression(batch, tokenizer))

# Evaluate the model on the test set
predictions = []
model.eval()
test_loss = 0.0
total_samples = 0

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']

        outputs = model(input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs.logits.squeeze(1), labels)  # Use appropriate loss function for regression

        test_loss += loss.item()
        predictions.append(outputs.logits.cpu().numpy())  # Append without extending
        total_samples += input_ids.size(0)

# Calculate average test loss
avg_test_loss = test_loss / len(test_loader)
print(f'Average Test Loss: {avg_test_loss:.4f}')

# Concatenate predictions without flattening
predictions = np.concatenate(predictions, axis=0)

# Ensure predictions cover the entire test dataset
predictions = predictions[:total_samples]

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(test_labels, predictions)
print(f'Mean Squared Error (MSE): {mse:.4f}')

# Plot actual vs predicted values

import torch
from transformers import BertTokenizer, BertForSequenceClassification

# Load BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)  # Assuming regression with a single output

# Load the saved model checkpoint
checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/fine')
model.load_state_dict(checkpoint['model_state_dict'])

def predict_expression(model, tokenizer, input_sequence):
    model.eval()
    with torch.no_grad():
        encoding = tokenizer(input_sequence, truncation=True, padding=True, return_tensors='pt')
        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()

        # Ensure input is in the correct shape (add batch dimension)
        input_ids = input_ids.unsqueeze(0)
        attention_mask = attention_mask.unsqueeze(0)

        outputs = model(input_ids, attention_mask=attention_mask)
        prediction = outputs.logits.item()  # Extract the single numerical prediction

    return prediction

input_sequence = "CCACAGAGGTCACTGAGGTCATCGGGCTGGACAGATGTCAACAGGAGGCCCTCAGAGCAGCCTGGCCTGGCCTAGTATACCTTGTAGTGTGAATCCTTCAT"  # Your input sequence here
predicted_output = predict_expression(model, tokenizer, input_sequence)
print("Predicted Output:", predicted_output)